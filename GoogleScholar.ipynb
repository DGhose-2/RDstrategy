{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# DATA IMPORT\n",
    "\n",
    "# your working directory for the code files\n",
    "import os\n",
    "cwd = os.getcwd()\n",
    "path = cwd + '\\\\Data'\n",
    "\n",
    "\n",
    "#Set file paths\n",
    "links_path = path + '\\\\projectPeopleLinks.csv'\n",
    "people_path = path + '\\\\people.csv'\n",
    "proj_path = path + '\\\\projects.csv'\n",
    "\n",
    "#Read in the files\n",
    "dfLinks = pd.read_csv(links_path)\n",
    "dfPeople = pd.read_csv(people_path)\n",
    "dfProj = pd.read_csv(proj_path)\n",
    "\n",
    "\n",
    "# We also need orgProjectTexts:\n",
    "path_int = cwd + '\\\\Intermediate_Files'\n",
    "orgProjectTexts_path = path_int + '\\\\orgProjectTexts.csv'\n",
    "orgProjectTexts = pd.read_csv(orgProjectTexts_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proj_people_links(df_links, df_people, df_proj):\n",
    "    # join people names\n",
    "    projectPeopleLinks = pd.merge(df_links, df_people, left_on=['personuuid'], right_on = ['personuuid'], how = 'left')\n",
    "    \n",
    "    # join project names and info\n",
    "    projectPeopleLinks = pd.merge(projectPeopleLinks, df_proj, left_on=['projectuuid'], right_on = ['projectuuid'], how = 'left')\n",
    "    \n",
    "    # combine the names columns and fill NAs\n",
    "    projectPeopleLinks['person_name'] = projectPeopleLinks[['firstname', 'othernames', 'surname']].stack().groupby(level=0).agg(' '.join)\n",
    "\n",
    "    # select columns to keep\n",
    "    projectPeopleLinks = projectPeopleLinks[['projectuuid', 'personuuid', 'title', 'role', 'person_name']].sort_values('title')\n",
    "    \n",
    "    return projectPeopleLinks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def researcher_base(people_links, proj_set=orgProjectTexts):\n",
    "    # remove rows where the UKRI dataset does not provide the proper name\n",
    "    people_links = people_links[people_links['person_name'] != \"Unknown Unknown Unknown\"]\n",
    "    \n",
    "    # remove rows where the project title has not been announced or recorded\n",
    "    people_links = people_links[people_links['title'] != \"TBC\"] \n",
    "\n",
    "    # reduce to only those titles in projectPeopleLinks which show up in orgProjectTexts\n",
    "    people_links_red = people_links[people_links['title'].isin(proj_set['title'])]\n",
    "    \n",
    "    # remove duplicates based on title and person_name\n",
    "    people_links_red = people_links_red.drop_duplicates(subset=[\"title\", \"person_name\"])\n",
    "    \n",
    "    # the \"Grants Team\" is not a researcher recognised by Google Scholar\n",
    "    people_links_red = people_links_red[people_links_red['person_name'] != \"Grants Team\"]\n",
    "    \n",
    "    # count number of researchers on project\n",
    "    people_links_red['people_count'] = people_links_red.groupby('title')['title'].transform('count')\n",
    "    \n",
    "    # prepare for Google Scholar title/abstract import\n",
    "    projectPeopleTexts = people_links_red.sort_values(['people_count', 'title'], ascending=[False, True])\n",
    "\n",
    "    # restrict to projects with 3+ researchers to match texts for\n",
    "    projectPeopleTexts_red = projectPeopleTexts[projectPeopleTexts['people_count'] >= 3]\n",
    "    \n",
    "    return projectPeopleTexts_red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projectPeopleLinks = proj_people_links(dfLinks, dfPeople, dfProj)\n",
    "\n",
    "projectPeopleTexts_red = researcher_base(projectPeopleLinks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Google Scholar:\n",
    "#If we want to replace the project titles with their research output:\n",
    "#Use projectPeopleLinks (perhaps use ProjectPeopleLinks-named.csv instead): first reduce to only those projectuuid in projectPeopleLinks which show up in orgProjectTexts (in projectuuid column). Then fill in the Google Scholar output as a separate column of this (the texts of titles just merged together).\n",
    "#Then, for each projectuuid in orgProjectTexts, you could look that up in projectPeopleLinks and append the row. Then de-duplicate (with ```drop_duplicates(subset=[\"orguuid\", \"projectuuid\"])```).\n",
    "\n",
    "#Note: for the sake of Google Scholar searching it may be most effective to actually remove the middle-names entirely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for use if search_after_banned=True:\n",
    "def new_proxy():\n",
    "    while True:\n",
    "        proxy = FreeProxy(rand=True, timeout=1).get()\n",
    "        proxy_works = scholarly.use_proxy(http=proxy, https=proxy)\n",
    "        if proxy_works:\n",
    "            break\n",
    "    print(proxy)\n",
    "    return proxy\n",
    "\n",
    "# collect titles from GoogleScholar, until you get reCAPTCHA'd\n",
    "def collect_research(AuthorList, min_authors=2, search_after_banned=False):\n",
    "    from scholarly import scholarly\n",
    "    \n",
    "    frames = []\n",
    "    \n",
    "    if search_after_banned==False:\n",
    "        for Author in AuthorList:\n",
    "            search_query = scholarly.search_author(Author)\n",
    "            try:\n",
    "                # query author on Google Scholar\n",
    "                author = next(search_query).fill()\n",
    "\n",
    "                # collect author's publications into Pandas dataframe\n",
    "                df = pd.DataFrame([x.__dict__ for x in author.publications])\n",
    "\n",
    "                # append author's name to distinguish their publications\n",
    "                df['author'] = Author\n",
    "\n",
    "                frames.append(df.copy())\n",
    "            except StopIteration:\n",
    "                # avoid error and move on if author is not looked up properly by Scholarly\n",
    "                pass\n",
    "    \n",
    "    elif search_after_banned==True:\n",
    "        new_proxy()\n",
    "\n",
    "        while True:\n",
    "            try:\n",
    "                for Author in AuthorList:\n",
    "                    search_query = scholarly.search_author(Author)\n",
    "                    author = next(search_query).fill()\n",
    "                    # creating DataFrame with authors\n",
    "                    df = pd.DataFrame([x.__dict__ for x in author.publications])\n",
    "                    df['author'] = Author\n",
    "                    frames.append(df.copy())\n",
    "                \n",
    "                # you will be waiting a long time! Nice to know it's finally worked\n",
    "                print(\"Got the results of the query\")\n",
    "                break\n",
    "                \n",
    "            except Exception as e:\n",
    "                set_new_proxy() \n",
    "        \n",
    "    # joining all author DataFrames\n",
    "    df = pd.concat(frames, axis=0)\n",
    "\n",
    "    # unpack bib into columns\n",
    "    df2 = pd.concat([df.drop(['bib'], axis=1), df['bib'].apply(pd.Series)], axis=1)\n",
    "\n",
    "    # counting unique authors attached to each title\n",
    "    df2['Titlematches'] = df2['title'].str.lower()\n",
    "    df_authors = df2.groupby('Titlematches').author.nunique()\n",
    "\n",
    "    output = df_authors[df_authors >= min_authors].index\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "AuthorList = ['Ross McMaster', 'Nicholas Alexander Monk', 'Julia Margaret Rees', 'William Zimmerman', 'Robert K Poole', 'J Green']\n",
    "author_output = collect_research(AuthorList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WARNING: the Scholarly API's title-based search gets caught by Google *really* quickly (as of 16 August 2020)\n",
    "\n",
    "def abstract_search(TitleList):\n",
    "    frames=[]\n",
    "\n",
    "    for Title in TitleList:\n",
    "        search_query = scholarly.search_pubs(Title)\n",
    "        pub_info = next(search_query)\n",
    "\n",
    "        df = pd.DataFrame([x.__dict__ for x in pub_info.bib])\n",
    "        df['title'] = Title\n",
    "        frames.append(df.copy())\n",
    "\n",
    "    # joining all title DataFrames\n",
    "    df = pd.concat(frames, axis=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
